{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of read csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "path = Path('./')\n",
    "\n",
    "submission = pd.read_csv(path / 'sample_submission.csv', index_col='id')\n",
    "labels = pd.read_csv(path / 'train_labels.csv', index_col='id')\n",
    "\n",
    "# the ids of the submission rows (useful later)\n",
    "sub_ids = submission.index\n",
    "\n",
    "# the ids of the labeled rows (useful later)\n",
    "gt_ids = labels.index \n",
    "\n",
    "# list of files in the submission folder\n",
    "subs = sorted(os.listdir(path / 'submission_files'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6222863195.csv : 0.6222863195\n"
     ]
    }
   ],
   "source": [
    "s0 = pd.read_csv(path / 'submission_files' / subs[0], index_col='id')\n",
    "\n",
    "score = log_loss(labels, s0.loc[gt_ids])\n",
    "\n",
    "# Notice the score of the labeled rows matches the file name\n",
    "print(subs[0],\":\", f'{score:.10f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6223807245.csv : 0.6222863195\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = pd.read_csv(path / 'submission_files' / subs[1], index_col='id')\n",
    "\n",
    "score = log_loss(labels, s0.loc[gt_ids])\n",
    "\n",
    "# Notice the score of the labeled rows matches the file name\n",
    "print(subs[1],\":\", f'{score:.10f}')\n",
    "len(s0.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge submission files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.44338393211365\n"
     ]
    }
   ],
   "source": [
    "np_preds = np.zeros((40000,len(subs)))\n",
    "df_preds = pd.DataFrame(np_preds)\n",
    "scores = []\n",
    "start = time.time()\n",
    "for i,sub in enumerate(subs):\n",
    "    pred = pd.read_csv(path / \"submission_files\" / sub, index_col = \"id\")\n",
    "    # np_preds[:,i] = pred[\"pred\"]\n",
    "    df_preds.iloc[:,i] = pred[\"pred\"]\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)\n",
    "np_preds = df_preds.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as tts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = np_preds[:len(labels),:]\n",
    "data_y = labels\n",
    "test_x = np_preds[len(labels):,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,val_x,train_y,val_y=tts(data_x,data_y,random_state=10,test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Inverse weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = np.array(scores)\n",
    "# df_inverse_weighting = df_preds * scores**2\n",
    "# df_inverse_weighting.sum(axis=1)/sum(scores**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np_preds = df_preds.to_numpy()\n",
    "# labels.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 6993, number of negative: 7007\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.277229 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1261109\n",
      "[LightGBM] [Info] Number of data points in the train set: 14000, number of used features: 4998\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499500 -> initscore=-0.002000\n",
      "[LightGBM] [Info] Start training from score -0.002000\n",
      "[1]\ttrain's binary_logloss: 0.662744\tvalid's binary_logloss: 0.667045\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[2]\ttrain's binary_logloss: 0.637524\tvalid's binary_logloss: 0.645224\n",
      "[3]\ttrain's binary_logloss: 0.616219\tvalid's binary_logloss: 0.627189\n",
      "[4]\ttrain's binary_logloss: 0.597863\tvalid's binary_logloss: 0.612368\n",
      "[5]\ttrain's binary_logloss: 0.582336\tvalid's binary_logloss: 0.600092\n",
      "[6]\ttrain's binary_logloss: 0.568485\tvalid's binary_logloss: 0.589665\n",
      "[7]\ttrain's binary_logloss: 0.556502\tvalid's binary_logloss: 0.580633\n",
      "[8]\ttrain's binary_logloss: 0.545928\tvalid's binary_logloss: 0.573211\n",
      "[9]\ttrain's binary_logloss: 0.536423\tvalid's binary_logloss: 0.567448\n",
      "[10]\ttrain's binary_logloss: 0.527905\tvalid's binary_logloss: 0.562008\n",
      "[11]\ttrain's binary_logloss: 0.520087\tvalid's binary_logloss: 0.557503\n",
      "[12]\ttrain's binary_logloss: 0.513173\tvalid's binary_logloss: 0.553852\n",
      "[13]\ttrain's binary_logloss: 0.506862\tvalid's binary_logloss: 0.551097\n",
      "[14]\ttrain's binary_logloss: 0.500819\tvalid's binary_logloss: 0.548141\n",
      "[15]\ttrain's binary_logloss: 0.495351\tvalid's binary_logloss: 0.545971\n",
      "[16]\ttrain's binary_logloss: 0.490285\tvalid's binary_logloss: 0.543778\n",
      "[17]\ttrain's binary_logloss: 0.485573\tvalid's binary_logloss: 0.542016\n",
      "[18]\ttrain's binary_logloss: 0.481155\tvalid's binary_logloss: 0.540147\n",
      "[19]\ttrain's binary_logloss: 0.476905\tvalid's binary_logloss: 0.538882\n",
      "[20]\ttrain's binary_logloss: 0.472996\tvalid's binary_logloss: 0.537913\n",
      "[21]\ttrain's binary_logloss: 0.469357\tvalid's binary_logloss: 0.536715\n",
      "[22]\ttrain's binary_logloss: 0.465515\tvalid's binary_logloss: 0.536233\n",
      "[23]\ttrain's binary_logloss: 0.46208\tvalid's binary_logloss: 0.535828\n",
      "[24]\ttrain's binary_logloss: 0.458457\tvalid's binary_logloss: 0.535146\n",
      "[25]\ttrain's binary_logloss: 0.455181\tvalid's binary_logloss: 0.534584\n",
      "[26]\ttrain's binary_logloss: 0.452095\tvalid's binary_logloss: 0.534027\n",
      "[27]\ttrain's binary_logloss: 0.448991\tvalid's binary_logloss: 0.533734\n",
      "[28]\ttrain's binary_logloss: 0.446135\tvalid's binary_logloss: 0.533551\n",
      "[29]\ttrain's binary_logloss: 0.443333\tvalid's binary_logloss: 0.53315\n",
      "[30]\ttrain's binary_logloss: 0.440541\tvalid's binary_logloss: 0.533197\n",
      "[31]\ttrain's binary_logloss: 0.437743\tvalid's binary_logloss: 0.533627\n",
      "[32]\ttrain's binary_logloss: 0.43477\tvalid's binary_logloss: 0.53323\n",
      "[33]\ttrain's binary_logloss: 0.431836\tvalid's binary_logloss: 0.533575\n",
      "[34]\ttrain's binary_logloss: 0.429259\tvalid's binary_logloss: 0.533619\n",
      "[35]\ttrain's binary_logloss: 0.426478\tvalid's binary_logloss: 0.533458\n",
      "[36]\ttrain's binary_logloss: 0.424097\tvalid's binary_logloss: 0.533472\n",
      "[37]\ttrain's binary_logloss: 0.421376\tvalid's binary_logloss: 0.533832\n",
      "[38]\ttrain's binary_logloss: 0.419072\tvalid's binary_logloss: 0.533912\n",
      "[39]\ttrain's binary_logloss: 0.416541\tvalid's binary_logloss: 0.53406\n",
      "[40]\ttrain's binary_logloss: 0.414026\tvalid's binary_logloss: 0.534137\n",
      "[41]\ttrain's binary_logloss: 0.411678\tvalid's binary_logloss: 0.534286\n",
      "[42]\ttrain's binary_logloss: 0.409455\tvalid's binary_logloss: 0.534314\n",
      "[43]\ttrain's binary_logloss: 0.40763\tvalid's binary_logloss: 0.534168\n",
      "[44]\ttrain's binary_logloss: 0.405444\tvalid's binary_logloss: 0.534278\n",
      "[45]\ttrain's binary_logloss: 0.403413\tvalid's binary_logloss: 0.534285\n",
      "[46]\ttrain's binary_logloss: 0.401071\tvalid's binary_logloss: 0.534368\n",
      "[47]\ttrain's binary_logloss: 0.399018\tvalid's binary_logloss: 0.534535\n",
      "[48]\ttrain's binary_logloss: 0.3968\tvalid's binary_logloss: 0.534837\n",
      "[49]\ttrain's binary_logloss: 0.394699\tvalid's binary_logloss: 0.534949\n",
      "Early stopping, best iteration is:\n",
      "[29]\ttrain's binary_logloss: 0.443333\tvalid's binary_logloss: 0.53315\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"objective\" : \"binary\",\n",
    "    \"metric\" : \"binary_logloss\",\n",
    "    \"random_state\":10\n",
    "}\n",
    "num_round = 100\n",
    "\n",
    "lgb_train = lgb.Dataset(train_x,train_y)\n",
    "lgb_val = lgb.Dataset(val_x,val_y)\n",
    "\n",
    "# train\n",
    "model = lgb.train(params,lgb_train,num_boost_round=num_round, valid_names=['train', 'valid']\n",
    "                  , valid_sets=[lgb_train, lgb_val], early_stopping_rounds=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blend = model.predict(test_x)\n",
    "submission[\"pred\"] = blend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.reset_index().to_csv(\"blend.csv\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier as xgbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, feature_types=None, gamma=0, gpu_id=-1,\n",
       "              grow_policy='depthwise', importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.300000012,\n",
       "              max_bin=256, max_cat_threshold=64, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=10, ...)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_xgb = xgbc(random_state=10)\n",
    "\n",
    "model_xgb.fit(np_preds[:len(labels),:],labels.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# model import \n",
    "\n",
    "def predict_cv(model,train_x,train_y,test_x):\n",
    "    preds = []\n",
    "    preds_test = []\n",
    "    va_idxes = []\n",
    "    \n",
    "    kf = KFold(n_splits = 4,shuffle=True,random_state=33)\n",
    "    \n",
    "    for i,(tr_idx,va_idx) in enumerate(kf.split(train_x)):\n",
    "        tr_x,va_x = train_x.iloc[tr_idx],train_x.iloc[va_idx]\n",
    "        tr_y,va_y = train_y.iloc[tr_idx],train_y.iloc[va_idx]\n",
    "        \n",
    "        model.fit(tr_x,tr_y,va_x,va_y)\n",
    "        pred = model.predict(va_x)\n",
    "        preds.append(pred)\n",
    "        pred_test = model.predict(test_x)\n",
    "        preds_test.append(pred_test)\n",
    "        va_idxes.append(va_idx)\n",
    "        \n",
    "    va_idxes = np.concatenate(va_idxes)\n",
    "    preds = cp.concatenate(preds,axis=0)\n",
    "    order = np.argsort(va_idxes)\n",
    "    pred_train = preds[order]\n",
    "    \n",
    "    \n",
    "    preds_test = np.mean(preds_test,axis = 0)\n",
    "    \n",
    "    return pred_train,preds_test\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier as rfs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create blending score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brending score =  0.6019399304\n"
     ]
    }
   ],
   "source": [
    "# blend = df_preds.mean(axis=1) #first submit\n",
    "# blend = df_inverse_weighting.sum(axis=1)/sum(scores**2) #second submit\n",
    "\n",
    "# third submit\n",
    "# medians_id = summary.query(\"diff >0.03\").index\n",
    "# blend = df_preds.mean(axis=1) #first submit\n",
    "# blend.iloc[medians_id] = summary.query(\"diff>0.03\")[\"diff\"]\n",
    "\n",
    "# fourth submit\n",
    "blend = np.sqrt(s0 * s1)\n",
    "\n",
    "score = log_loss(labels, blend.loc[gt_ids])\n",
    "\n",
    "print(\"Brending score = \", f'{score:.10f}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
